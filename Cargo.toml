
[workspace]
members = [
    "crates/llm-*",
]
resolver = "2"

[workspace.package]
name = "ferrous-llm"
version = "0.1.0"
description = "LLM library"
authors = ["Andre Roelofs <andre@eurora-labs.com>"]
license = "Apache-2.0"
edition = "2024"

[workspace.dependencies]
tokio = { version = "1.45.1", default-features = false, features = ["full"] }
serde = "1.0"
serde_json = "1.0"
chrono = { version = "0.4", features = ["serde"] }
dotenv = "0.15"
futures = "0.3"
thiserror = "2.0.12"

llm-core = { path = "crates/llm-core", version = "*"}
llm-memory = { path = "crates/llm-memory", version = "*" }
llm-ollama = { path = "crates/llm-ollama", version = "*" }
llm-anthropic = { path = "crates/llm-anthropic", version = "*" }
llm-openai = { path = "crates/llm-openai", version = "*" }

[features]
default = []
openai = ["llm-openai"]
ollama = ["llm-ollama"]
anthropic = ["llm-anthropic"]

# Add workspace-level package for e2e tests
[package]
name = "llm-workspace"
version.workspace = true
edition.workspace = true

[dependencies]
llm-core = { workspace = true }
llm-openai = { workspace = true, optional = true }
llm-ollama = { workspace = true, optional = true }
llm-anthropic = { workspace = true, optional = true }
dotenv.workspace = true
tokio.workspace = true
futures.workspace = true
serde_json.workspace = true
chrono.workspace = true
thiserror.workspace = true


[profile.release]
codegen-units = 1 # Compile crates one after another so the compiler can optimize better
lto = true        # Enables link to optimizations
opt-level = "s"   # Optimize for binary size
debug = true      # Enable debug symbols, for profiling

[profile.bench]
codegen-units = 256
lto = false
opt-level = 3

[profile.dev]
incremental = true
codegen-units = 256
overflow-checks = false

